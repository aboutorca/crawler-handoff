# Historical Crawler - Code Walkthrough

## File Structure
- **File**: `backend/src/services/crawlers/historical-crawler.js`
- **Lines of Code**: 2,732
- **Classes**: 4 main classes
- **Dependencies**: Puppeteer, Supabase, UUID

## 1. Configuration Section (Lines 1-86)

### Environment-Based Configuration
```javascript
// Lines 20-21: Date range from environment variables
const START_YEAR = parseInt(process.env.CRAWLER_START_YEAR || '2010');
const END_YEAR = parseInt(process.env.CRAWLER_END_YEAR || '2024');

// Lines 36-78: Main configuration object
const HISTORICAL_CONFIG = {
  START_YEAR: START_YEAR,
  END_YEAR: END_YEAR,
  MAX_WORKERS: 30,  // Parallel Chrome instances
  CHUNK_SIZE: 1500,  // Text chunk size for vector DB
  CHUNK_OVERLAP: 200,  // Context preservation

  // Database connection (Lines 76-77)
  SUPABASE_URL: process.env.SUPABASE_URL,
  SUPABASE_ANON_KEY: process.env.SUPABASE_ANON_KEY
};
```

## 2. Crash Recovery System (Lines 95-214)

### Checkpoint Management
```javascript
class CrashRecoveryManager {
  // Lines 98-99: Checkpoint file for resuming
  constructor() {
    this.checkpointFile = './historical-crawler-checkpoint.json';
  }

  // Lines 106-122: Save progress periodically
  async saveCheckpoint(state) {
    const checkpoint = {
      timestamp: new Date().toISOString(),
      documentsProcessed: state.documentsProcessed,
      currentYear: state.currentYear,
      lastProcessedCase: state.lastCase
    };
    fs.writeFileSync(this.checkpointFile, JSON.stringify(checkpoint));
  }
}
```

## 3. Database Operations (Lines 217-848)

### SupabaseUploader Class
```javascript
class SupabaseUploader {
  // Lines 228-257: Case creation with duplicate handling
  async ensureCaseExists(caseInfo) {
    // Check cache first (Line 229)
    if (this.caseCache.has(caseInfo.caseNumber)) {
      return this.caseCache.get(caseInfo.caseNumber);
    }

    // Check database (Lines 233-237)
    const { data: existingCase } = await this.supabase
      .from('cases')
      .select('id')
      .eq('case_number', caseInfo.caseNumber)
      .single();

    // Create if doesn't exist (Lines 244-256)
    if (!existingCase) {
      const { data: newCase } = await this.supabase
        .from('cases')
        .insert({
          case_number: caseInfo.caseNumber,
          company: caseInfo.company,
          // ... other fields
        });
    }
  }

  // Lines 350-447: Document upload with text chunking
  async uploadDocument(documentData) {
    // Check for duplicates (Lines 367-374)
    const { data: existingDoc } = await this.supabase
      .from('documents')
      .select('id')
      .eq('document_url', documentData.url)
      .single();

    if (existingDoc) {
      return { success: true, duplicate: true };
    }

    // Create chunks with overlap (Lines 650-687)
    const chunks = this.createChunksWithOverlap(
      extractedText,
      HISTORICAL_CONFIG.CHUNK_SIZE,
      HISTORICAL_CONFIG.CHUNK_OVERLAP
    );
  }
}
```

## 4. Web Scraping Logic (Lines 851-1190)

### DocumentExtractor Class
```javascript
class DocumentExtractor {
  // Lines 865-900: Detect PDF viewer type
  async detectViewerType(page) {
    // Check for LaserFiche viewer
    const laserFicheElements = await page.$$('.laserfiche-container');
    if (laserFicheElements.length > 0) {
      return 'laserfiche';
    }

    // Check for WebLink viewer
    const iframeElements = await page.$$('iframe#content');
    if (iframeElements.length > 0) {
      return 'weblink';
    }

    return 'unknown';
  }

  // Lines 950-1050: Progressive retry system
  async extractWithProgressiveTiming(page, documentUrl) {
    const attempts = HISTORICAL_CONFIG.SPEED.PROGRESSIVE_TIMING.WEBLINK_ATTEMPTS;

    for (const attempt of attempts) {
      try {
        console.log(`Trying ${attempt.name} timing (${attempt.wait}ms wait)`);
        await page.waitForTimeout(attempt.wait);

        // Try extraction
        const text = await this.extractFromWebLink(page);
        if (text && text.length > 100) {
          return text;
        }
      } catch (error) {
        console.log(`${attempt.name} attempt failed, trying next...`);
      }
    }
  }
}
```

## 5. Main Crawler Class (Lines 1193-2732)

### HistoricalCrawler - Orchestration
```javascript
class HistoricalCrawler {
  // Lines 1195-1200: Initialize components
  constructor() {
    this.uploader = new SupabaseUploader();
    this.crashRecovery = new CrashRecoveryManager();
    this.blacklistedDocuments = new Set();
    this.documentQueue = [];
    this.activeWorkers = 0;
  }

  // Lines 1350-1450: Case discovery from website
  async discoverCasesFromListing(page, url, utilityType) {
    await page.goto(url, { waitUntil: 'networkidle2' });

    // Parse case listings (Lines 1360-1390)
    const cases = await page.evaluate(() => {
      const rows = document.querySelectorAll('tr.case-row');
      return Array.from(rows).map(row => ({
        caseNumber: row.querySelector('.case-number')?.textContent,
        company: row.querySelector('.company')?.textContent,
        dateFiled: row.querySelector('.date')?.textContent,
        description: row.querySelector('.description')?.textContent
      }));
    });

    // Filter by date range (Lines 1400-1420)
    return cases.filter(caseInfo => {
      const caseYear = new Date(caseInfo.dateFiled).getFullYear();
      return caseYear >= HISTORICAL_CONFIG.START_YEAR &&
             caseYear <= HISTORICAL_CONFIG.END_YEAR;
    });
  }

  // Lines 1500-1650: Worker pool management
  async processDocumentQueue() {
    const workers = [];

    // Launch workers up to MAX_WORKERS (Lines 1510-1530)
    for (let i = 0; i < HISTORICAL_CONFIG.MAX_WORKERS; i++) {
      workers.push(this.documentWorker(i));
    }

    // Wait for all workers to complete
    await Promise.all(workers);
  }

  // Lines 1700-1850: Individual worker logic
  async documentWorker(workerId) {
    const browser = await puppeteer.launch({
      headless: 'new',
      args: ['--no-sandbox', '--disable-setuid-sandbox']
    });

    try {
      while (this.documentQueue.length > 0) {
        const document = this.documentQueue.shift();

        // Process document (Lines 1750-1800)
        const page = await browser.newPage();
        const extractor = new DocumentExtractor(browser, this.uploader);

        await extractor.extractDocumentWithRetry(
          document.url,
          document.metadata
        );

        await page.close();
      }
    } finally {
      await browser.close();
    }
  }

  // Lines 2500-2700: Main entry point
  async crawlHistoricalData() {
    console.log('ðŸš€ Starting Historical Crawler');
    console.log(`ðŸ“… Date Range: ${START_YEAR} - ${END_YEAR}`);
    console.log(`ðŸ‘· Workers: ${HISTORICAL_CONFIG.MAX_WORKERS}`);

    try {
      // Phase 1: Discovery
      await this.discoverAllCases();

      // Phase 2: Document extraction
      await this.processDocumentQueue();

      // Phase 3: Completion
      console.log('âœ… Crawling complete!');

    } catch (error) {
      // Save checkpoint for recovery
      await this.crashRecovery.saveCheckpoint(this.state);
      throw error;
    }
  }
}
```

## 6. Text Processing (Lines 650-687)

### Chunking Algorithm
```javascript
// Create overlapping chunks for better context in vector search
createChunksWithOverlap(text, chunkSize = 1500, overlap = 200) {
  const chunks = [];
  let position = 0;

  while (position < text.length) {
    // Extract chunk with size limit
    let endPosition = Math.min(position + chunkSize, text.length);

    // Try to break at sentence boundary
    if (endPosition < text.length) {
      const lastPeriod = text.lastIndexOf('.', endPosition);
      if (lastPeriod > position + chunkSize - 200) {
        endPosition = lastPeriod + 1;
      }
    }

    chunks.push({
      content: text.slice(position, endPosition),
      index: chunks.length,
      start: position,
      end: endPosition
    });

    // Move position with overlap
    position = endPosition - overlap;
  }

  return chunks;
}
```

## 7. Rate Limiting & Politeness

### Request Spacing
```javascript
// Lines 44-65: Progressive timing configuration
PROGRESSIVE_TIMING: {
  // Start fast, slow down if needed
  WEBLINK_ATTEMPTS: [
    { wait: 300, name: "fast" },     // 300ms initial wait
    { wait: 600, name: "medium" },   // 600ms if fast fails
    { wait: 1200, name: "slow" }     // 1.2s for stubborn documents
  ]
}

// Lines 1750-1760: Delay between navigations
await page.goto(documentUrl, {
  waitUntil: 'networkidle2',
  timeout: 30000
});
await page.waitForTimeout(HISTORICAL_CONFIG.SPEED.WEBLINK_PAGE_WAIT);
```

## Key Design Patterns

### 1. Worker Pool Pattern
- Manages parallel browser instances
- Prevents resource exhaustion
- Enables high throughput

### 2. Progressive Retry Pattern
- Adapts to server response times
- Prevents unnecessary delays
- Maximizes success rate

### 3. Cache-Aside Pattern
- In-memory cache for database lookups
- Reduces database load
- Speeds up duplicate detection

### 4. Checkpoint Pattern
- Enables crash recovery
- Prevents data loss
- Allows incremental processing

## Performance Characteristics

| Metric | Value | Notes |
|--------|-------|-------|
| Documents/Hour | ~675 | With 30 workers |
| Success Rate | 99.5% | With retry system |
| Memory Usage | 3-4GB | 30 Chrome instances |
| Network Usage | 50-100 Mbps | Peak processing |
| Database Writes | ~100/min | Batched inserts |

## Error Handling Examples

```javascript
// Lines 260-275: Race condition handling
if (error.code === '23505' || error.message.includes('duplicate key')) {
  console.log('Race condition detected - checking if exists');
  // Another worker created it, fetch and continue
  const { data: raceCase } = await this.supabase
    .from('cases')
    .select('id')
    .eq('case_number', caseInfo.caseNumber)
    .single();
  return raceCase.id;
}

// Lines 1594-1610: Blacklisting after failures
if (retryCount >= 3) {
  this.blacklistedDocuments.add(documentUrl);
  fs.appendFileSync('blacklisted-documents.json',
    JSON.stringify({ url: documentUrl, reason: error.message })
  );
}
```

---

*This walkthrough covers the main components of the 2,732-line crawler system. Each section is designed to be independently testable and maintainable.*
