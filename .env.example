# Idaho PUC Document Crawler - Environment Configuration
# Copy this file to .env and fill in your values

# ==================== REQUIRED VARIABLES ====================
# These MUST be configured for the crawler to function

# Supabase Database Configuration
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your-supabase-anon-key-here
# Alternative: Use service role key if anon key not available
# SUPABASE_SERVICE_ROLE_KEY=your-service-role-key-here

# OpenAI API Configuration (Required for embeddings in nightly-crawler)
OPENAI_API_KEY=sk-your-openai-api-key-here

# ==================== OPTIONAL CRAWLER SETTINGS ====================
# Adjust these based on your needs and infrastructure

# Date Range Configuration (Historical Crawler)
CRAWLER_START_YEAR=2010  # Default: 2010 - Earliest year to crawl
CRAWLER_END_YEAR=2024    # Default: 2024 - Latest year to crawl

# Performance Tuning
CRAWLER_MAX_WORKERS=30   # Default: 30 - Number of parallel Chrome instances
                         # Reduce if experiencing memory issues (each worker ~100-150MB RAM)
NIGHTLY_MAX_WORKERS=5    # Default: 5 - Reduced workers for scheduled runs
                         # Lower value = less resource usage for daily updates

# Feature Flags
SKIP_EMBEDDINGS=false    # Default: false - Set to true to skip embedding generation
                         # Useful for testing document extraction without API costs

# Environment Detection
NODE_ENV=development     # Options: development, production
RAILWAY_ENVIRONMENT=     # Set by Railway.app automatically in production

# ==================== PRODUCTION NOTES ====================
#
# Memory Requirements:
# - With 30 workers: ~4GB RAM
# - With 10 workers: ~1.5GB RAM
# - Minimum recommended: 2GB RAM
#
# API Rate Limits:
# - OpenAI Tier 1: 3,000 requests/min, 1M tokens/min
# - Supabase: Check your plan's limits
#
# Typical Usage:
# - Historical crawl (2010-2024): ~6-8 hours first run
# - Nightly updates: ~10-30 minutes
# - Document processing rate: ~450 docs/40 minutes